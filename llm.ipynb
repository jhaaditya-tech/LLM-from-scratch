{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Step 1: Creating Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of characters:  20479\n",
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no great surprise to me to hear that, in the height of his glory, he had dropped his painting, married a rich widow, and established himself in a villa on the Riviera. (Though I rather thought it would have been Rome or Florence.)\n",
      "\n",
      "\"The height of his glory\"--that was what the women called it. I can hear Mrs. Gideon Thwing--his last Chicago sitter--deploring his unaccountable abdication. \"Of course it's going to send the value of my picture 'way up; but I don't think of that, Mr. Rickham--the loss to Arrt is all I think of.\" The word, on Mrs. Thwing's lips, multiplied its _rs_ as though they were reflected in an endless vista of mirrors. And it was not only the Mrs. Thwings who mourned. Had not the exquisite Hermia Croft, at the last Grafton Gallery show, stopped me before Gisburn's \"Moon-dancers\" to say, with tears in her eyes: \"We shall not look upon its like again\"?\n",
      "\n",
      "Well!--even through th\n"
     ]
    }
   ],
   "source": [
    "#Reading from book, the Verdict\n",
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "#Total number of character print and print the first 1000 characters\n",
    "print(\"The total number of characters: \", len(raw_text))\n",
    "print(raw_text[:1000])\n",
    "\n",
    "#Goal is to tokenize all the character is the book to the individual words and special characters that we will then turn into embedding for LLM training\n",
    "\n",
    "#The LLM we are traing wil be on single book due to the hardware limitation, just using single book to illustrate the concept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello,', ' ', 'world!', ' ', 'This', ' ', 'is', ' ', 'a', ' ', 'test.']\n",
      "['Hello', ',', '', ' ', 'world', '!', '', ' ', 'This', ' ', 'is', ' ', 'a', ' ', 'test', '.', '']\n",
      "['Hello', ',', 'world', '!', 'This', 'is', 'a', 'test', '.']\n",
      "['Hello', ',', 'world', '!', 'Is', 'This', '-', '-', 'a', 'test', '?']\n",
      "['Hello', ',', 'world', '!', 'Is', 'This', '-', '-', 'a', 'test', '?']\n"
     ]
    }
   ],
   "source": [
    "#How can we best split the tect to obtain a list of tokens?\n",
    "#We can use the regular expression operations to split the text into tokens. This will split the text into words and special characters \n",
    "# based on white space and punctuation marks\n",
    "\n",
    "\n",
    "import re\n",
    "text = \"Hello, world! This is a test.\"\n",
    "#Split wherever white spaces are found\n",
    "result = re.split(r'(\\s)', text)\n",
    "print(result)\n",
    "\n",
    "#We will modify the regular expression splits on whitespace (\\s) and punctuation marks ([\\s.,!?:;])\n",
    "result = re.split(r'([\\s.,!?:;])', text)\n",
    "print(result)\n",
    "\n",
    "#The issue is our list still includes whitespace characters, we can remove them by filtering out empty strings\n",
    "#item.strip will only be true when there is no whitespace\n",
    "result = [token for token in result if token.strip()]\n",
    "print(result)\n",
    "\n",
    "#Remove white space or not?\n",
    "''' When develping a tokenizer, whether we should encode whitesspaces as separate character is based on the application requirements.\n",
    "Removing whitespaces reducte the memorry and compuing requirements.\n",
    "However, keeping whitespces can be useful, if we train model that are  senstive to the exact structure of the text (for eg,, Python code\n",
    "which is sensitive to indentation and spacing). It makes sense to keep the white space here for the training of the LLM'''\n",
    "\n",
    "#We will modify to include all the punctuations that can be found in the text\n",
    "#Two line of code where we build the tokenizer and filter out the empty strings\n",
    "text = \"Hello, world! Is This-- a test?\"\n",
    "result = re.split(r'([\\s.,!?:;()--])', text)\n",
    "result = [token.strip() for token in result if token.strip()]\n",
    "print(result)\n",
    "\n",
    "#Strio whitespace from each item and then filter out whtespace from any string\n",
    "result = [token for token in result if token.strip()]\n",
    "print(result)\n",
    "\n",
    "#For building LLM different tokenizer scheme is used which is called byte pair encoding (BPE).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in', 'the', 'height', 'of', 'his', 'glory', ',', 'he', 'had', 'dropped', 'his', 'painting', ',', 'married', 'a', 'rich', 'widow', ',', 'and', 'established', 'himself', 'in', 'a', 'villa', 'on', 'the', 'Riviera', '.', '(', 'Though', 'I', 'rather', 'thought', 'it', 'would', 'have', 'been', 'Rome', 'or', 'Florence', '.', ')', '\"', 'The', 'height', 'of', 'his', 'glory', '\"', '--', 'that', 'was', 'what', 'the', 'women', 'called', 'it', '.', 'I', 'can', 'hear', 'Mrs', '.', 'Gideon', 'Thwing', '--', 'his', 'last', 'Chicago', 'sitter', '--']\n",
      "4690\n"
     ]
    }
   ],
   "source": [
    "#Apply the toekenization in entire raw text\n",
    "preprocessed_text = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\n",
    "preprocessed_text = [token.strip() for token in preprocessed_text if token.strip()]\n",
    "print(preprocessed_text[:100])\n",
    "print(len(preprocessed_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Creating Token IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1130\n"
     ]
    }
   ],
   "source": [
    "#Now creating the list of all unique tokens and sort them alphabetically to determine the vcabulary size\n",
    "\n",
    "all_words = sorted(set(preprocessed_text))\n",
    "vocab_size = len(all_words)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'!': 0, '\"': 1, \"'\": 2, '(': 3, ')': 4, ',': 5, '--': 6, '.': 7, ':': 8, ';': 9, '?': 10, 'A': 11, 'Ah': 12, 'Among': 13, 'And': 14, 'Are': 15, 'Arrt': 16, 'As': 17, 'At': 18, 'Be': 19, 'Begin': 20, 'Burlington': 21, 'But': 22, 'By': 23, 'Carlo': 24, 'Chicago': 25, 'Claude': 26, 'Come': 27, 'Croft': 28, 'Destroyed': 29, 'Devonshire': 30, 'Don': 31, 'Dubarry': 32, 'Emperors': 33, 'Florence': 34, 'For': 35, 'Gallery': 36, 'Gideon': 37, 'Gisburn': 38, 'Gisburns': 39, 'Grafton': 40, 'Greek': 41, 'Grindle': 42, 'Grindles': 43, 'HAD': 44, 'Had': 45, 'Hang': 46, 'Has': 47, 'He': 48, 'Her': 49, 'Hermia': 50, 'His': 51, 'How': 52, 'I': 53, 'If': 54, 'In': 55, 'It': 56, 'Jack': 57, 'Jove': 58, 'Just': 59, 'Lord': 60, 'Made': 61, 'Miss': 62, 'Money': 63, 'Monte': 64, 'Moon-dancers': 65, 'Mr': 66, 'Mrs': 67, 'My': 68, 'Never': 69, 'No': 70, 'Now': 71, 'Nutley': 72, 'Of': 73, 'Oh': 74, 'On': 75, 'Once': 76, 'Only': 77, 'Or': 78, 'Perhaps': 79, 'Poor': 80, 'Professional': 81, 'Renaissance': 82, 'Rickham': 83, 'Riviera': 84, 'Rome': 85, 'Russian': 86, 'Sevres': 87, 'She': 88, 'Stroud': 89, 'Strouds': 90, 'Suddenly': 91, 'That': 92, 'The': 93, 'Then': 94, 'There': 95, 'They': 96, 'This': 97, 'Those': 98, 'Though': 99, 'Thwing': 100, 'Thwings': 101, 'To': 102, 'Usually': 103, 'Venetian': 104, 'Victor': 105, 'Was': 106, 'We': 107, 'Well': 108, 'What': 109, 'When': 110, 'Why': 111, 'Yes': 112, 'You': 113, '_': 114, 'a': 115, 'abdication': 116, 'able': 117, 'about': 118, 'above': 119, 'abruptly': 120, 'absolute': 121, 'absorbed': 122, 'absurdity': 123, 'academic': 124, 'accuse': 125, 'accustomed': 126, 'across': 127, 'activity': 128, 'add': 129, 'added': 130, 'admirers': 131, 'adopted': 132, 'adulation': 133, 'advance': 134, 'aesthetic': 135, 'affect': 136, 'afraid': 137, 'after': 138, 'afterward': 139, 'again': 140, 'ago': 141, 'ah': 142, 'air': 143, 'alive': 144, 'all': 145, 'almost': 146, 'alone': 147, 'along': 148, 'always': 149, 'am': 150, 'amazement': 151, 'amid': 152, 'among': 153, 'amplest': 154, 'amusing': 155, 'an': 156, 'and': 157, 'another': 158, 'answer': 159, 'answered': 160, 'any': 161, 'anything': 162, 'anywhere': 163, 'apparent': 164, 'apparently': 165, 'appearance': 166, 'appeared': 167, 'appointed': 168, 'are': 169, 'arm': 170, 'arm-chair': 171, 'arm-chairs': 172, 'arms': 173, 'art': 174, 'articles': 175, 'artist': 176, 'as': 177, 'aside': 178, 'asked': 179, 'at': 180, 'atmosphere': 181, 'atom': 182, 'attack': 183, 'attention': 184, 'attitude': 185, 'audacities': 186, 'away': 187, 'awful': 188, 'axioms': 189, 'azaleas': 190, 'back': 191, 'background': 192, 'balance': 193, 'balancing': 194, 'balustraded': 195, 'basking': 196, 'bath-rooms': 197, 'be': 198, 'beaming': 199, 'bean-stalk': 200, 'bear': 201, 'beard': 202, 'beauty': 203, 'became': 204, 'because': 205, 'becoming': 206, 'bed': 207, 'been': 208, 'before': 209, 'began': 210, 'begun': 211, 'behind': 212, 'being': 213, 'believed': 214, 'beneath': 215, 'bespoke': 216, 'better': 217, 'between': 218, 'big': 219, 'bits': 220, 'bitterness': 221, 'blocked': 222, 'born': 223, 'borne': 224, 'boudoir': 225, 'bravura': 226, 'break': 227, 'breaking': 228, 'breathing': 229, 'bric-a-brac': 230, 'briefly': 231, 'brings': 232, 'bronzes': 233, 'brought': 234, 'brown': 235, 'brush': 236, 'bull': 237, 'business': 238, 'but': 239, 'buying': 240, 'by': 241, 'called': 242, 'came': 243, 'can': 244, 'canvas': 245, 'canvases': 246, 'cards': 247, 'care': 248, 'career': 249, 'caught': 250, 'central': 251, 'chair': 252, 'chap': 253, 'characteristic': 254, 'charming': 255, 'cheap': 256, 'check': 257, 'cheeks': 258, 'chest': 259, 'chimney-piece': 260, 'chucked': 261, 'cigar': 262, 'cigarette': 263, 'cigars': 264, 'circulation': 265, 'circumstance': 266, 'circus-clown': 267, 'claimed': 268, 'clasping': 269, 'clear': 270, 'cleverer': 271, 'close': 272, 'clue': 273, 'coat': 274, 'collapsed': 275, 'colour': 276, 'come': 277, 'comfortable': 278, 'coming': 279, 'companion': 280, 'compared': 281, 'complex': 282, 'confident': 283, 'congesting': 284, 'conjugal': 285, 'constraint': 286, 'consummate': 287, 'contended': 288, 'continued': 289, 'corner': 290, 'corrected': 291, 'could': 292, 'couldn': 293, 'count': 294, 'countenance': 295, 'couple': 296, 'course': 297, 'covered': 298, 'craft': 299, 'cried': 300, 'crossed': 301, 'crowned': 302, 'crumbled': 303, 'cry': 304, 'cured': 305, 'curiosity': 306, 'curious': 307, 'current': 308, 'curtains': 309, 'd': 310, 'dabble': 311, 'damask': 312, 'dark': 313, 'dashed': 314, 'day': 315, 'days': 316, 'dead': 317, 'deadening': 318, 'dear': 319, 'deep': 320, 'deerhound': 321, 'degree': 322, 'delicate': 323, 'demand': 324, 'denied': 325, 'deploring': 326, 'deprecating': 327, 'deprecatingly': 328, 'desire': 329, 'destroyed': 330, 'destruction': 331, 'desultory': 332, 'detail': 333, 'diagnosis': 334, 'did': 335, 'didn': 336, 'died': 337, 'dim': 338, 'dimmest': 339, 'dingy': 340, 'dining-room': 341, 'disarming': 342, 'discovery': 343, 'discrimination': 344, 'discussion': 345, 'disdain': 346, 'disdained': 347, 'disease': 348, 'disguised': 349, 'display': 350, 'dissatisfied': 351, 'distinguished': 352, 'distract': 353, 'divert': 354, 'do': 355, 'doesn': 356, 'doing': 357, 'domestic': 358, 'don': 359, 'done': 360, 'donkey': 361, 'down': 362, 'dozen': 363, 'dragged': 364, 'drawing-room': 365, 'drawing-rooms': 366, 'drawn': 367, 'dress-closets': 368, 'drew': 369, 'dropped': 370, 'each': 371, 'earth': 372, 'ease': 373, 'easel': 374, 'easy': 375, 'echoed': 376, 'economy': 377, 'effect': 378, 'effects': 379, 'efforts': 380, 'egregious': 381, 'eighteenth-century': 382, 'elbow': 383, 'elegant': 384, 'else': 385, 'embarrassed': 386, 'enabled': 387, 'end': 388, 'endless': 389, 'enjoy': 390, 'enlightenment': 391, 'enough': 392, 'ensuing': 393, 'equally': 394, 'equanimity': 395, 'escape': 396, 'established': 397, 'etching': 398, 'even': 399, 'event': 400, 'ever': 401, 'everlasting': 402, 'every': 403, 'exasperated': 404, 'except': 405, 'excuse': 406, 'excusing': 407, 'existed': 408, 'expected': 409, 'exquisite': 410, 'exquisitely': 411, 'extenuation': 412, 'exterminating': 413, 'extracting': 414, 'eye': 415, 'eyebrows': 416, 'eyes': 417, 'face': 418, 'faces': 419, 'fact': 420, 'faded': 421, 'failed': 422, 'failure': 423, 'fair': 424, 'faith': 425, 'false': 426, 'familiar': 427, 'famille-verte': 428, 'fancy': 429, 'fashionable': 430, 'fate': 431, 'feather': 432, 'feet': 433, 'fell': 434, 'fellow': 435, 'felt': 436, 'few': 437, 'fewer': 438, 'finality': 439, 'find': 440, 'fingers': 441, 'first': 442, 'fit': 443, 'fitting': 444, 'five': 445, 'flash': 446, 'flashed': 447, 'florid': 448, 'flowers': 449, 'fluently': 450, 'flung': 451, 'follow': 452, 'followed': 453, 'fond': 454, 'footstep': 455, 'for': 456, 'forced': 457, 'forcing': 458, 'forehead': 459, 'foreign': 460, 'foreseen': 461, 'forgive': 462, 'forgotten': 463, 'form': 464, 'formed': 465, 'forming': 466, 'forward': 467, 'fostered': 468, 'found': 469, 'foundations': 470, 'fragment': 471, 'fragments': 472, 'frame': 473, 'frames': 474, 'frequently': 475, 'friend': 476, 'from': 477, 'full': 478, 'fullest': 479, 'furiously': 480, 'furrowed': 481, 'garlanded': 482, 'garlands': 483, 'gave': 484, 'genial': 485, 'genius': 486, 'gesture': 487, 'get': 488, 'getting': 489, 'give': 490, 'given': 491, 'glad': 492, 'glanced': 493, 'glimpse': 494, 'gloried': 495, 'glory': 496, 'go': 497, 'going': 498, 'gone': 499, 'good': 500, 'good-breeding': 501, 'good-humoured': 502, 'got': 503, 'grace': 504, 'gradually': 505, 'gray': 506, 'grayish': 507, 'great': 508, 'greatest': 509, 'greatness': 510, 'grew': 511, 'groping': 512, 'growing': 513, 'had': 514, 'hadn': 515, 'hair': 516, 'half': 517, 'half-light': 518, 'half-mechanically': 519, 'hall': 520, 'hand': 521, 'hands': 522, 'handsome': 523, 'hanging': 524, 'happen': 525, 'happened': 526, 'hard': 527, 'hardly': 528, 'has': 529, 'have': 530, 'haven': 531, 'having': 532, 'he': 533, 'head': 534, 'hear': 535, 'heard': 536, 'heart': 537, 'height': 538, 'her': 539, 'here': 540, 'hermit': 541, 'herself': 542, 'hesitations': 543, 'hide': 544, 'high': 545, 'him': 546, 'himself': 547, 'hint': 548, 'his': 549, 'history': 550, 'holding': 551, 'home': 552, 'honour': 553, 'hooded': 554, 'hostess': 555, 'hot-house': 556, 'hour': 557, 'hours': 558, 'house': 559, 'how': 560, 'hung': 561, 'husband': 562, 'idea': 563, 'idle': 564, 'idling': 565, 'if': 566, 'immediately': 567, 'in': 568, 'incense': 569, 'indifferent': 570, 'inevitable': 571, 'inevitably': 572, 'inflexible': 573, 'insensible': 574, 'insignificant': 575, 'instinctively': 576, 'instructive': 577, 'interesting': 578, 'into': 579, 'ironic': 580, 'irony': 581, 'irrelevance': 582, 'irrevocable': 583, 'is': 584, 'it': 585, 'its': 586, 'itself': 587, 'jardiniere': 588, 'jealousy': 589, 'just': 590, 'keep': 591, 'kept': 592, 'kind': 593, 'knees': 594, 'knew': 595, 'know': 596, 'known': 597, 'laid': 598, 'lair': 599, 'landing': 600, 'language': 601, 'last': 602, 'late': 603, 'later': 604, 'latter': 605, 'laugh': 606, 'laughed': 607, 'lay': 608, 'leading': 609, 'lean': 610, 'learned': 611, 'least': 612, 'leathery': 613, 'leave': 614, 'led': 615, 'left': 616, 'leisure': 617, 'lends': 618, 'lent': 619, 'let': 620, 'lies': 621, 'life': 622, 'life-likeness': 623, 'lift': 624, 'lifted': 625, 'light': 626, 'lightly': 627, 'like': 628, 'liked': 629, 'line': 630, 'lines': 631, 'lingered': 632, 'lips': 633, 'lit': 634, 'little': 635, 'live': 636, 'll': 637, 'loathing': 638, 'long': 639, 'longed': 640, 'longer': 641, 'look': 642, 'looked': 643, 'looking': 644, 'lose': 645, 'loss': 646, 'lounging': 647, 'lovely': 648, 'lucky': 649, 'lump': 650, 'luncheon-table': 651, 'luxury': 652, 'lying': 653, 'made': 654, 'make': 655, 'man': 656, 'manage': 657, 'managed': 658, 'mantel-piece': 659, 'marble': 660, 'married': 661, 'may': 662, 'me': 663, 'meant': 664, 'mediocrity': 665, 'medium': 666, 'mentioned': 667, 'mere': 668, 'merely': 669, 'met': 670, 'might': 671, 'mighty': 672, 'millionaire': 673, 'mine': 674, 'minute': 675, 'minutes': 676, 'mirrors': 677, 'modest': 678, 'modesty': 679, 'moment': 680, 'money': 681, 'monumental': 682, 'mood': 683, 'morbidly': 684, 'more': 685, 'most': 686, 'mourn': 687, 'mourned': 688, 'moustache': 689, 'moved': 690, 'much': 691, 'muddling': 692, 'multiplied': 693, 'murmur': 694, 'muscles': 695, 'must': 696, 'my': 697, 'myself': 698, 'mysterious': 699, 'naive': 700, 'near': 701, 'nearly': 702, 'negatived': 703, 'nervous': 704, 'nervousness': 705, 'neutral': 706, 'never': 707, 'next': 708, 'no': 709, 'none': 710, 'not': 711, 'note': 712, 'nothing': 713, 'now': 714, 'nymphs': 715, 'oak': 716, 'obituary': 717, 'object': 718, 'objects': 719, 'occurred': 720, 'oddly': 721, 'of': 722, 'off': 723, 'often': 724, 'oh': 725, 'old': 726, 'on': 727, 'once': 728, 'one': 729, 'ones': 730, 'only': 731, 'onto': 732, 'open': 733, 'or': 734, 'other': 735, 'our': 736, 'ourselves': 737, 'out': 738, 'outline': 739, 'oval': 740, 'over': 741, 'own': 742, 'packed': 743, 'paid': 744, 'paint': 745, 'painted': 746, 'painter': 747, 'painting': 748, 'pale': 749, 'paled': 750, 'palm-trees': 751, 'panel': 752, 'panelling': 753, 'pardonable': 754, 'pardoned': 755, 'part': 756, 'passages': 757, 'passing': 758, 'past': 759, 'pastels': 760, 'pathos': 761, 'patient': 762, 'people': 763, 'perceptible': 764, 'perfect': 765, 'persistence': 766, 'persuasively': 767, 'phrase': 768, 'picture': 769, 'pictures': 770, 'pines': 771, 'pink': 772, 'place': 773, 'placed': 774, 'plain': 775, 'platitudes': 776, 'pleased': 777, 'pockets': 778, 'point': 779, 'poised': 780, 'poor': 781, 'portrait': 782, 'posing': 783, 'possessed': 784, 'poverty': 785, 'predicted': 786, 'preliminary': 787, 'presenting': 788, 'prestidigitation': 789, 'pretty': 790, 'previous': 791, 'price': 792, 'pride': 793, 'princely': 794, 'prism': 795, 'problem': 796, 'proclaiming': 797, 'prodigious': 798, 'profusion': 799, 'protest': 800, 'prove': 801, 'public': 802, 'purblind': 803, 'purely': 804, 'pushed': 805, 'put': 806, 'qualities': 807, 'quality': 808, 'queerly': 809, 'question': 810, 'quickly': 811, 'quietly': 812, 'quite': 813, 'quote': 814, 'rain': 815, 'raised': 816, 'random': 817, 'rather': 818, 're': 819, 'real': 820, 'really': 821, 'reared': 822, 'reason': 823, 'reassurance': 824, 'recovering': 825, 'recreated': 826, 'reflected': 827, 'reflection': 828, 'regrets': 829, 'relatively': 830, 'remained': 831, 'remember': 832, 'reminded': 833, 'repeating': 834, 'represented': 835, 'reproduction': 836, 'resented': 837, 'resolve': 838, 'resources': 839, 'rest': 840, 'rich': 841, 'ridiculous': 842, 'robbed': 843, 'romantic': 844, 'room': 845, 'rose': 846, 'rs': 847, 'rule': 848, 'run': 849, 's': 850, 'said': 851, 'same': 852, 'satisfaction': 853, 'savour': 854, 'saw': 855, 'say': 856, 'saying': 857, 'says': 858, 'scorn': 859, 'scornful': 860, 'secret': 861, 'see': 862, 'seemed': 863, 'seen': 864, 'self-confident': 865, 'send': 866, 'sensation': 867, 'sensitive': 868, 'sent': 869, 'serious': 870, 'set': 871, 'sex': 872, 'shade': 873, 'shaking': 874, 'shall': 875, 'she': 876, 'shirked': 877, 'short': 878, 'should': 879, 'shoulder': 880, 'shoulders': 881, 'show': 882, 'showed': 883, 'showy': 884, 'shrug': 885, 'shrugged': 886, 'sight': 887, 'sign': 888, 'silent': 889, 'silver': 890, 'similar': 891, 'simpleton': 892, 'simplifications': 893, 'simply': 894, 'since': 895, 'single': 896, 'sitter': 897, 'sitters': 898, 'sketch': 899, 'skill': 900, 'slight': 901, 'slightly': 902, 'slowly': 903, 'small': 904, 'smile': 905, 'smiling': 906, 'sneer': 907, 'so': 908, 'solace': 909, 'some': 910, 'somebody': 911, 'something': 912, 'spacious': 913, 'spaniel': 914, 'speaking-tubes': 915, 'speculations': 916, 'spite': 917, 'splash': 918, 'square': 919, 'stairs': 920, 'stammer': 921, 'stand': 922, 'standing': 923, 'started': 924, 'stay': 925, 'still': 926, 'stocked': 927, 'stood': 928, 'stopped': 929, 'stopping': 930, 'straddling': 931, 'straight': 932, 'strain': 933, 'straining': 934, 'strange': 935, 'straw': 936, 'stream': 937, 'stroke': 938, 'strokes': 939, 'strolled': 940, 'strongest': 941, 'strongly': 942, 'struck': 943, 'studio': 944, 'stuff': 945, 'subject': 946, 'substantial': 947, 'suburban': 948, 'such': 949, 'suddenly': 950, 'suffered': 951, 'sugar': 952, 'suggested': 953, 'sunburn': 954, 'sunburnt': 955, 'sunlit': 956, 'superb': 957, 'sure': 958, 'surest': 959, 'surface': 960, 'surprise': 961, 'surprised': 962, 'surrounded': 963, 'suspected': 964, 'sweetly': 965, 'sweetness': 966, 'swelling': 967, 'swept': 968, 'swum': 969, 't': 970, 'table': 971, 'take': 972, 'taken': 973, 'talking': 974, 'tea': 975, 'tears': 976, 'technicalities': 977, 'technique': 978, 'tell': 979, 'tells': 980, 'tempting': 981, 'terra-cotta': 982, 'terrace': 983, 'terraces': 984, 'terribly': 985, 'than': 986, 'that': 987, 'the': 988, 'their': 989, 'them': 990, 'then': 991, 'there': 992, 'therefore': 993, 'they': 994, 'thin': 995, 'thing': 996, 'things': 997, 'think': 998, 'this': 999, 'thither': 1000, 'those': 1001, 'though': 1002, 'thought': 1003, 'three': 1004, 'threshold': 1005, 'threw': 1006, 'through': 1007, 'throwing': 1008, 'tie': 1009, 'till': 1010, 'time': 1011, 'timorously': 1012, 'tinge': 1013, 'tips': 1014, 'tired': 1015, 'to': 1016, 'told': 1017, 'tone': 1018, 'tones': 1019, 'too': 1020, 'took': 1021, 'tottering': 1022, 'touched': 1023, 'toward': 1024, 'trace': 1025, 'trade': 1026, 'transmute': 1027, 'traps': 1028, 'travelled': 1029, 'tribute': 1030, 'tributes': 1031, 'tricks': 1032, 'tried': 1033, 'trouser-presses': 1034, 'true': 1035, 'truth': 1036, 'turned': 1037, 'twenty': 1038, 'twenty-four': 1039, 'twice': 1040, 'twirling': 1041, 'unaccountable': 1042, 'uncertain': 1043, 'under': 1044, 'underlay': 1045, 'underneath': 1046, 'understand': 1047, 'unexpected': 1048, 'untouched': 1049, 'unusual': 1050, 'up': 1051, 'up-stream': 1052, 'upon': 1053, 'upset': 1054, 'upstairs': 1055, 'us': 1056, 'used': 1057, 'usual': 1058, 'value': 1059, 'varnishing': 1060, 'vases': 1061, 've': 1062, 'veins': 1063, 'velveteen': 1064, 'very': 1065, 'villa': 1066, 'vindicated': 1067, 'virtuosity': 1068, 'vista': 1069, 'vocation': 1070, 'voice': 1071, 'wall': 1072, 'wander': 1073, 'want': 1074, 'wanted': 1075, 'wants': 1076, 'was': 1077, 'wasn': 1078, 'watched': 1079, 'watching': 1080, 'water-colour': 1081, 'waves': 1082, 'way': 1083, 'weekly': 1084, 'weeks': 1085, 'welcome': 1086, 'went': 1087, 'were': 1088, 'what': 1089, 'when': 1090, 'whenever': 1091, 'where': 1092, 'which': 1093, 'while': 1094, 'white': 1095, 'white-panelled': 1096, 'who': 1097, 'whole': 1098, 'whom': 1099, 'why': 1100, 'wide': 1101, 'widow': 1102, 'wife': 1103, 'wild': 1104, 'wincing': 1105, 'window-curtains': 1106, 'wish': 1107, 'with': 1108, 'without': 1109, 'wits': 1110, 'woman': 1111, 'women': 1112, 'won': 1113, 'wonder': 1114, 'wondered': 1115, 'word': 1116, 'work': 1117, 'working': 1118, 'worth': 1119, 'would': 1120, 'wouldn': 1121, 'year': 1122, 'years': 1123, 'yellow': 1124, 'yet': 1125, 'you': 1126, 'younger': 1127, 'your': 1128, 'yourself': 1129}\n",
      "('!', 0)\n",
      "('\"', 1)\n",
      "(\"'\", 2)\n",
      "('(', 3)\n",
      "(')', 4)\n",
      "(',', 5)\n",
      "('--', 6)\n",
      "('.', 7)\n",
      "(':', 8)\n",
      "(';', 9)\n",
      "('?', 10)\n",
      "('A', 11)\n",
      "('Ah', 12)\n",
      "('Among', 13)\n",
      "('And', 14)\n",
      "('Are', 15)\n",
      "('Arrt', 16)\n",
      "('As', 17)\n",
      "('At', 18)\n",
      "('Be', 19)\n",
      "('Begin', 20)\n",
      "('Burlington', 21)\n",
      "('But', 22)\n",
      "('By', 23)\n",
      "('Carlo', 24)\n",
      "('Chicago', 25)\n",
      "('Claude', 26)\n",
      "('Come', 27)\n",
      "('Croft', 28)\n",
      "('Destroyed', 29)\n",
      "('Devonshire', 30)\n",
      "('Don', 31)\n",
      "('Dubarry', 32)\n",
      "('Emperors', 33)\n",
      "('Florence', 34)\n",
      "('For', 35)\n",
      "('Gallery', 36)\n",
      "('Gideon', 37)\n",
      "('Gisburn', 38)\n",
      "('Gisburns', 39)\n",
      "('Grafton', 40)\n",
      "('Greek', 41)\n",
      "('Grindle', 42)\n",
      "('Grindles', 43)\n",
      "('HAD', 44)\n",
      "('Had', 45)\n",
      "('Hang', 46)\n",
      "('Has', 47)\n",
      "('He', 48)\n",
      "('Her', 49)\n",
      "('Hermia', 50)\n"
     ]
    }
   ],
   "source": [
    "#After determining the voacab size, we can now assign a unique integer to each token in the vocabulary  using a dictionary  data structure\n",
    "#We will create a dictionary that maps each token to a unique integer   and vice versa\n",
    "word_to_id = {word: i for i, word in enumerate(all_words)}\n",
    "print(word_to_id)\n",
    "\n",
    "for i, item in enumerate(word_to_id.items()):\n",
    "    print(item)\n",
    "    if i >= 50:\n",
    "        break\n",
    "\n",
    "#Now the dictionay will contain the individual tokens associated with unique integers labels\n",
    "#This process is encoding, later we need decoder to convert from the token id to the word itshelf, to give output in word form, basically also called reverse mapping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '!', 1: '\"', 2: \"'\", 3: '(', 4: ')', 5: ',', 6: '--', 7: '.', 8: ':', 9: ';', 10: '?', 11: 'A', 12: 'Ah', 13: 'Among', 14: 'And', 15: 'Are', 16: 'Arrt', 17: 'As', 18: 'At', 19: 'Be', 20: 'Begin', 21: 'Burlington', 22: 'But', 23: 'By', 24: 'Carlo', 25: 'Chicago', 26: 'Claude', 27: 'Come', 28: 'Croft', 29: 'Destroyed', 30: 'Devonshire', 31: 'Don', 32: 'Dubarry', 33: 'Emperors', 34: 'Florence', 35: 'For', 36: 'Gallery', 37: 'Gideon', 38: 'Gisburn', 39: 'Gisburns', 40: 'Grafton', 41: 'Greek', 42: 'Grindle', 43: 'Grindles', 44: 'HAD', 45: 'Had', 46: 'Hang', 47: 'Has', 48: 'He', 49: 'Her', 50: 'Hermia', 51: 'His', 52: 'How', 53: 'I', 54: 'If', 55: 'In', 56: 'It', 57: 'Jack', 58: 'Jove', 59: 'Just', 60: 'Lord', 61: 'Made', 62: 'Miss', 63: 'Money', 64: 'Monte', 65: 'Moon-dancers', 66: 'Mr', 67: 'Mrs', 68: 'My', 69: 'Never', 70: 'No', 71: 'Now', 72: 'Nutley', 73: 'Of', 74: 'Oh', 75: 'On', 76: 'Once', 77: 'Only', 78: 'Or', 79: 'Perhaps', 80: 'Poor', 81: 'Professional', 82: 'Renaissance', 83: 'Rickham', 84: 'Riviera', 85: 'Rome', 86: 'Russian', 87: 'Sevres', 88: 'She', 89: 'Stroud', 90: 'Strouds', 91: 'Suddenly', 92: 'That', 93: 'The', 94: 'Then', 95: 'There', 96: 'They', 97: 'This', 98: 'Those', 99: 'Though', 100: 'Thwing', 101: 'Thwings', 102: 'To', 103: 'Usually', 104: 'Venetian', 105: 'Victor', 106: 'Was', 107: 'We', 108: 'Well', 109: 'What', 110: 'When', 111: 'Why', 112: 'Yes', 113: 'You', 114: '_', 115: 'a', 116: 'abdication', 117: 'able', 118: 'about', 119: 'above', 120: 'abruptly', 121: 'absolute', 122: 'absorbed', 123: 'absurdity', 124: 'academic', 125: 'accuse', 126: 'accustomed', 127: 'across', 128: 'activity', 129: 'add', 130: 'added', 131: 'admirers', 132: 'adopted', 133: 'adulation', 134: 'advance', 135: 'aesthetic', 136: 'affect', 137: 'afraid', 138: 'after', 139: 'afterward', 140: 'again', 141: 'ago', 142: 'ah', 143: 'air', 144: 'alive', 145: 'all', 146: 'almost', 147: 'alone', 148: 'along', 149: 'always', 150: 'am', 151: 'amazement', 152: 'amid', 153: 'among', 154: 'amplest', 155: 'amusing', 156: 'an', 157: 'and', 158: 'another', 159: 'answer', 160: 'answered', 161: 'any', 162: 'anything', 163: 'anywhere', 164: 'apparent', 165: 'apparently', 166: 'appearance', 167: 'appeared', 168: 'appointed', 169: 'are', 170: 'arm', 171: 'arm-chair', 172: 'arm-chairs', 173: 'arms', 174: 'art', 175: 'articles', 176: 'artist', 177: 'as', 178: 'aside', 179: 'asked', 180: 'at', 181: 'atmosphere', 182: 'atom', 183: 'attack', 184: 'attention', 185: 'attitude', 186: 'audacities', 187: 'away', 188: 'awful', 189: 'axioms', 190: 'azaleas', 191: 'back', 192: 'background', 193: 'balance', 194: 'balancing', 195: 'balustraded', 196: 'basking', 197: 'bath-rooms', 198: 'be', 199: 'beaming', 200: 'bean-stalk', 201: 'bear', 202: 'beard', 203: 'beauty', 204: 'became', 205: 'because', 206: 'becoming', 207: 'bed', 208: 'been', 209: 'before', 210: 'began', 211: 'begun', 212: 'behind', 213: 'being', 214: 'believed', 215: 'beneath', 216: 'bespoke', 217: 'better', 218: 'between', 219: 'big', 220: 'bits', 221: 'bitterness', 222: 'blocked', 223: 'born', 224: 'borne', 225: 'boudoir', 226: 'bravura', 227: 'break', 228: 'breaking', 229: 'breathing', 230: 'bric-a-brac', 231: 'briefly', 232: 'brings', 233: 'bronzes', 234: 'brought', 235: 'brown', 236: 'brush', 237: 'bull', 238: 'business', 239: 'but', 240: 'buying', 241: 'by', 242: 'called', 243: 'came', 244: 'can', 245: 'canvas', 246: 'canvases', 247: 'cards', 248: 'care', 249: 'career', 250: 'caught', 251: 'central', 252: 'chair', 253: 'chap', 254: 'characteristic', 255: 'charming', 256: 'cheap', 257: 'check', 258: 'cheeks', 259: 'chest', 260: 'chimney-piece', 261: 'chucked', 262: 'cigar', 263: 'cigarette', 264: 'cigars', 265: 'circulation', 266: 'circumstance', 267: 'circus-clown', 268: 'claimed', 269: 'clasping', 270: 'clear', 271: 'cleverer', 272: 'close', 273: 'clue', 274: 'coat', 275: 'collapsed', 276: 'colour', 277: 'come', 278: 'comfortable', 279: 'coming', 280: 'companion', 281: 'compared', 282: 'complex', 283: 'confident', 284: 'congesting', 285: 'conjugal', 286: 'constraint', 287: 'consummate', 288: 'contended', 289: 'continued', 290: 'corner', 291: 'corrected', 292: 'could', 293: 'couldn', 294: 'count', 295: 'countenance', 296: 'couple', 297: 'course', 298: 'covered', 299: 'craft', 300: 'cried', 301: 'crossed', 302: 'crowned', 303: 'crumbled', 304: 'cry', 305: 'cured', 306: 'curiosity', 307: 'curious', 308: 'current', 309: 'curtains', 310: 'd', 311: 'dabble', 312: 'damask', 313: 'dark', 314: 'dashed', 315: 'day', 316: 'days', 317: 'dead', 318: 'deadening', 319: 'dear', 320: 'deep', 321: 'deerhound', 322: 'degree', 323: 'delicate', 324: 'demand', 325: 'denied', 326: 'deploring', 327: 'deprecating', 328: 'deprecatingly', 329: 'desire', 330: 'destroyed', 331: 'destruction', 332: 'desultory', 333: 'detail', 334: 'diagnosis', 335: 'did', 336: 'didn', 337: 'died', 338: 'dim', 339: 'dimmest', 340: 'dingy', 341: 'dining-room', 342: 'disarming', 343: 'discovery', 344: 'discrimination', 345: 'discussion', 346: 'disdain', 347: 'disdained', 348: 'disease', 349: 'disguised', 350: 'display', 351: 'dissatisfied', 352: 'distinguished', 353: 'distract', 354: 'divert', 355: 'do', 356: 'doesn', 357: 'doing', 358: 'domestic', 359: 'don', 360: 'done', 361: 'donkey', 362: 'down', 363: 'dozen', 364: 'dragged', 365: 'drawing-room', 366: 'drawing-rooms', 367: 'drawn', 368: 'dress-closets', 369: 'drew', 370: 'dropped', 371: 'each', 372: 'earth', 373: 'ease', 374: 'easel', 375: 'easy', 376: 'echoed', 377: 'economy', 378: 'effect', 379: 'effects', 380: 'efforts', 381: 'egregious', 382: 'eighteenth-century', 383: 'elbow', 384: 'elegant', 385: 'else', 386: 'embarrassed', 387: 'enabled', 388: 'end', 389: 'endless', 390: 'enjoy', 391: 'enlightenment', 392: 'enough', 393: 'ensuing', 394: 'equally', 395: 'equanimity', 396: 'escape', 397: 'established', 398: 'etching', 399: 'even', 400: 'event', 401: 'ever', 402: 'everlasting', 403: 'every', 404: 'exasperated', 405: 'except', 406: 'excuse', 407: 'excusing', 408: 'existed', 409: 'expected', 410: 'exquisite', 411: 'exquisitely', 412: 'extenuation', 413: 'exterminating', 414: 'extracting', 415: 'eye', 416: 'eyebrows', 417: 'eyes', 418: 'face', 419: 'faces', 420: 'fact', 421: 'faded', 422: 'failed', 423: 'failure', 424: 'fair', 425: 'faith', 426: 'false', 427: 'familiar', 428: 'famille-verte', 429: 'fancy', 430: 'fashionable', 431: 'fate', 432: 'feather', 433: 'feet', 434: 'fell', 435: 'fellow', 436: 'felt', 437: 'few', 438: 'fewer', 439: 'finality', 440: 'find', 441: 'fingers', 442: 'first', 443: 'fit', 444: 'fitting', 445: 'five', 446: 'flash', 447: 'flashed', 448: 'florid', 449: 'flowers', 450: 'fluently', 451: 'flung', 452: 'follow', 453: 'followed', 454: 'fond', 455: 'footstep', 456: 'for', 457: 'forced', 458: 'forcing', 459: 'forehead', 460: 'foreign', 461: 'foreseen', 462: 'forgive', 463: 'forgotten', 464: 'form', 465: 'formed', 466: 'forming', 467: 'forward', 468: 'fostered', 469: 'found', 470: 'foundations', 471: 'fragment', 472: 'fragments', 473: 'frame', 474: 'frames', 475: 'frequently', 476: 'friend', 477: 'from', 478: 'full', 479: 'fullest', 480: 'furiously', 481: 'furrowed', 482: 'garlanded', 483: 'garlands', 484: 'gave', 485: 'genial', 486: 'genius', 487: 'gesture', 488: 'get', 489: 'getting', 490: 'give', 491: 'given', 492: 'glad', 493: 'glanced', 494: 'glimpse', 495: 'gloried', 496: 'glory', 497: 'go', 498: 'going', 499: 'gone', 500: 'good', 501: 'good-breeding', 502: 'good-humoured', 503: 'got', 504: 'grace', 505: 'gradually', 506: 'gray', 507: 'grayish', 508: 'great', 509: 'greatest', 510: 'greatness', 511: 'grew', 512: 'groping', 513: 'growing', 514: 'had', 515: 'hadn', 516: 'hair', 517: 'half', 518: 'half-light', 519: 'half-mechanically', 520: 'hall', 521: 'hand', 522: 'hands', 523: 'handsome', 524: 'hanging', 525: 'happen', 526: 'happened', 527: 'hard', 528: 'hardly', 529: 'has', 530: 'have', 531: 'haven', 532: 'having', 533: 'he', 534: 'head', 535: 'hear', 536: 'heard', 537: 'heart', 538: 'height', 539: 'her', 540: 'here', 541: 'hermit', 542: 'herself', 543: 'hesitations', 544: 'hide', 545: 'high', 546: 'him', 547: 'himself', 548: 'hint', 549: 'his', 550: 'history', 551: 'holding', 552: 'home', 553: 'honour', 554: 'hooded', 555: 'hostess', 556: 'hot-house', 557: 'hour', 558: 'hours', 559: 'house', 560: 'how', 561: 'hung', 562: 'husband', 563: 'idea', 564: 'idle', 565: 'idling', 566: 'if', 567: 'immediately', 568: 'in', 569: 'incense', 570: 'indifferent', 571: 'inevitable', 572: 'inevitably', 573: 'inflexible', 574: 'insensible', 575: 'insignificant', 576: 'instinctively', 577: 'instructive', 578: 'interesting', 579: 'into', 580: 'ironic', 581: 'irony', 582: 'irrelevance', 583: 'irrevocable', 584: 'is', 585: 'it', 586: 'its', 587: 'itself', 588: 'jardiniere', 589: 'jealousy', 590: 'just', 591: 'keep', 592: 'kept', 593: 'kind', 594: 'knees', 595: 'knew', 596: 'know', 597: 'known', 598: 'laid', 599: 'lair', 600: 'landing', 601: 'language', 602: 'last', 603: 'late', 604: 'later', 605: 'latter', 606: 'laugh', 607: 'laughed', 608: 'lay', 609: 'leading', 610: 'lean', 611: 'learned', 612: 'least', 613: 'leathery', 614: 'leave', 615: 'led', 616: 'left', 617: 'leisure', 618: 'lends', 619: 'lent', 620: 'let', 621: 'lies', 622: 'life', 623: 'life-likeness', 624: 'lift', 625: 'lifted', 626: 'light', 627: 'lightly', 628: 'like', 629: 'liked', 630: 'line', 631: 'lines', 632: 'lingered', 633: 'lips', 634: 'lit', 635: 'little', 636: 'live', 637: 'll', 638: 'loathing', 639: 'long', 640: 'longed', 641: 'longer', 642: 'look', 643: 'looked', 644: 'looking', 645: 'lose', 646: 'loss', 647: 'lounging', 648: 'lovely', 649: 'lucky', 650: 'lump', 651: 'luncheon-table', 652: 'luxury', 653: 'lying', 654: 'made', 655: 'make', 656: 'man', 657: 'manage', 658: 'managed', 659: 'mantel-piece', 660: 'marble', 661: 'married', 662: 'may', 663: 'me', 664: 'meant', 665: 'mediocrity', 666: 'medium', 667: 'mentioned', 668: 'mere', 669: 'merely', 670: 'met', 671: 'might', 672: 'mighty', 673: 'millionaire', 674: 'mine', 675: 'minute', 676: 'minutes', 677: 'mirrors', 678: 'modest', 679: 'modesty', 680: 'moment', 681: 'money', 682: 'monumental', 683: 'mood', 684: 'morbidly', 685: 'more', 686: 'most', 687: 'mourn', 688: 'mourned', 689: 'moustache', 690: 'moved', 691: 'much', 692: 'muddling', 693: 'multiplied', 694: 'murmur', 695: 'muscles', 696: 'must', 697: 'my', 698: 'myself', 699: 'mysterious', 700: 'naive', 701: 'near', 702: 'nearly', 703: 'negatived', 704: 'nervous', 705: 'nervousness', 706: 'neutral', 707: 'never', 708: 'next', 709: 'no', 710: 'none', 711: 'not', 712: 'note', 713: 'nothing', 714: 'now', 715: 'nymphs', 716: 'oak', 717: 'obituary', 718: 'object', 719: 'objects', 720: 'occurred', 721: 'oddly', 722: 'of', 723: 'off', 724: 'often', 725: 'oh', 726: 'old', 727: 'on', 728: 'once', 729: 'one', 730: 'ones', 731: 'only', 732: 'onto', 733: 'open', 734: 'or', 735: 'other', 736: 'our', 737: 'ourselves', 738: 'out', 739: 'outline', 740: 'oval', 741: 'over', 742: 'own', 743: 'packed', 744: 'paid', 745: 'paint', 746: 'painted', 747: 'painter', 748: 'painting', 749: 'pale', 750: 'paled', 751: 'palm-trees', 752: 'panel', 753: 'panelling', 754: 'pardonable', 755: 'pardoned', 756: 'part', 757: 'passages', 758: 'passing', 759: 'past', 760: 'pastels', 761: 'pathos', 762: 'patient', 763: 'people', 764: 'perceptible', 765: 'perfect', 766: 'persistence', 767: 'persuasively', 768: 'phrase', 769: 'picture', 770: 'pictures', 771: 'pines', 772: 'pink', 773: 'place', 774: 'placed', 775: 'plain', 776: 'platitudes', 777: 'pleased', 778: 'pockets', 779: 'point', 780: 'poised', 781: 'poor', 782: 'portrait', 783: 'posing', 784: 'possessed', 785: 'poverty', 786: 'predicted', 787: 'preliminary', 788: 'presenting', 789: 'prestidigitation', 790: 'pretty', 791: 'previous', 792: 'price', 793: 'pride', 794: 'princely', 795: 'prism', 796: 'problem', 797: 'proclaiming', 798: 'prodigious', 799: 'profusion', 800: 'protest', 801: 'prove', 802: 'public', 803: 'purblind', 804: 'purely', 805: 'pushed', 806: 'put', 807: 'qualities', 808: 'quality', 809: 'queerly', 810: 'question', 811: 'quickly', 812: 'quietly', 813: 'quite', 814: 'quote', 815: 'rain', 816: 'raised', 817: 'random', 818: 'rather', 819: 're', 820: 'real', 821: 'really', 822: 'reared', 823: 'reason', 824: 'reassurance', 825: 'recovering', 826: 'recreated', 827: 'reflected', 828: 'reflection', 829: 'regrets', 830: 'relatively', 831: 'remained', 832: 'remember', 833: 'reminded', 834: 'repeating', 835: 'represented', 836: 'reproduction', 837: 'resented', 838: 'resolve', 839: 'resources', 840: 'rest', 841: 'rich', 842: 'ridiculous', 843: 'robbed', 844: 'romantic', 845: 'room', 846: 'rose', 847: 'rs', 848: 'rule', 849: 'run', 850: 's', 851: 'said', 852: 'same', 853: 'satisfaction', 854: 'savour', 855: 'saw', 856: 'say', 857: 'saying', 858: 'says', 859: 'scorn', 860: 'scornful', 861: 'secret', 862: 'see', 863: 'seemed', 864: 'seen', 865: 'self-confident', 866: 'send', 867: 'sensation', 868: 'sensitive', 869: 'sent', 870: 'serious', 871: 'set', 872: 'sex', 873: 'shade', 874: 'shaking', 875: 'shall', 876: 'she', 877: 'shirked', 878: 'short', 879: 'should', 880: 'shoulder', 881: 'shoulders', 882: 'show', 883: 'showed', 884: 'showy', 885: 'shrug', 886: 'shrugged', 887: 'sight', 888: 'sign', 889: 'silent', 890: 'silver', 891: 'similar', 892: 'simpleton', 893: 'simplifications', 894: 'simply', 895: 'since', 896: 'single', 897: 'sitter', 898: 'sitters', 899: 'sketch', 900: 'skill', 901: 'slight', 902: 'slightly', 903: 'slowly', 904: 'small', 905: 'smile', 906: 'smiling', 907: 'sneer', 908: 'so', 909: 'solace', 910: 'some', 911: 'somebody', 912: 'something', 913: 'spacious', 914: 'spaniel', 915: 'speaking-tubes', 916: 'speculations', 917: 'spite', 918: 'splash', 919: 'square', 920: 'stairs', 921: 'stammer', 922: 'stand', 923: 'standing', 924: 'started', 925: 'stay', 926: 'still', 927: 'stocked', 928: 'stood', 929: 'stopped', 930: 'stopping', 931: 'straddling', 932: 'straight', 933: 'strain', 934: 'straining', 935: 'strange', 936: 'straw', 937: 'stream', 938: 'stroke', 939: 'strokes', 940: 'strolled', 941: 'strongest', 942: 'strongly', 943: 'struck', 944: 'studio', 945: 'stuff', 946: 'subject', 947: 'substantial', 948: 'suburban', 949: 'such', 950: 'suddenly', 951: 'suffered', 952: 'sugar', 953: 'suggested', 954: 'sunburn', 955: 'sunburnt', 956: 'sunlit', 957: 'superb', 958: 'sure', 959: 'surest', 960: 'surface', 961: 'surprise', 962: 'surprised', 963: 'surrounded', 964: 'suspected', 965: 'sweetly', 966: 'sweetness', 967: 'swelling', 968: 'swept', 969: 'swum', 970: 't', 971: 'table', 972: 'take', 973: 'taken', 974: 'talking', 975: 'tea', 976: 'tears', 977: 'technicalities', 978: 'technique', 979: 'tell', 980: 'tells', 981: 'tempting', 982: 'terra-cotta', 983: 'terrace', 984: 'terraces', 985: 'terribly', 986: 'than', 987: 'that', 988: 'the', 989: 'their', 990: 'them', 991: 'then', 992: 'there', 993: 'therefore', 994: 'they', 995: 'thin', 996: 'thing', 997: 'things', 998: 'think', 999: 'this', 1000: 'thither', 1001: 'those', 1002: 'though', 1003: 'thought', 1004: 'three', 1005: 'threshold', 1006: 'threw', 1007: 'through', 1008: 'throwing', 1009: 'tie', 1010: 'till', 1011: 'time', 1012: 'timorously', 1013: 'tinge', 1014: 'tips', 1015: 'tired', 1016: 'to', 1017: 'told', 1018: 'tone', 1019: 'tones', 1020: 'too', 1021: 'took', 1022: 'tottering', 1023: 'touched', 1024: 'toward', 1025: 'trace', 1026: 'trade', 1027: 'transmute', 1028: 'traps', 1029: 'travelled', 1030: 'tribute', 1031: 'tributes', 1032: 'tricks', 1033: 'tried', 1034: 'trouser-presses', 1035: 'true', 1036: 'truth', 1037: 'turned', 1038: 'twenty', 1039: 'twenty-four', 1040: 'twice', 1041: 'twirling', 1042: 'unaccountable', 1043: 'uncertain', 1044: 'under', 1045: 'underlay', 1046: 'underneath', 1047: 'understand', 1048: 'unexpected', 1049: 'untouched', 1050: 'unusual', 1051: 'up', 1052: 'up-stream', 1053: 'upon', 1054: 'upset', 1055: 'upstairs', 1056: 'us', 1057: 'used', 1058: 'usual', 1059: 'value', 1060: 'varnishing', 1061: 'vases', 1062: 've', 1063: 'veins', 1064: 'velveteen', 1065: 'very', 1066: 'villa', 1067: 'vindicated', 1068: 'virtuosity', 1069: 'vista', 1070: 'vocation', 1071: 'voice', 1072: 'wall', 1073: 'wander', 1074: 'want', 1075: 'wanted', 1076: 'wants', 1077: 'was', 1078: 'wasn', 1079: 'watched', 1080: 'watching', 1081: 'water-colour', 1082: 'waves', 1083: 'way', 1084: 'weekly', 1085: 'weeks', 1086: 'welcome', 1087: 'went', 1088: 'were', 1089: 'what', 1090: 'when', 1091: 'whenever', 1092: 'where', 1093: 'which', 1094: 'while', 1095: 'white', 1096: 'white-panelled', 1097: 'who', 1098: 'whole', 1099: 'whom', 1100: 'why', 1101: 'wide', 1102: 'widow', 1103: 'wife', 1104: 'wild', 1105: 'wincing', 1106: 'window-curtains', 1107: 'wish', 1108: 'with', 1109: 'without', 1110: 'wits', 1111: 'woman', 1112: 'women', 1113: 'won', 1114: 'wonder', 1115: 'wondered', 1116: 'word', 1117: 'work', 1118: 'working', 1119: 'worth', 1120: 'would', 1121: 'wouldn', 1122: 'year', 1123: 'years', 1124: 'yellow', 1125: 'yet', 1126: 'you', 1127: 'younger', 1128: 'your', 1129: 'yourself'}\n"
     ]
    }
   ],
   "source": [
    "#For decoder, we create the reverse mapping of the dictionary. This will map toekn Ids back to the corresponding text tokens\n",
    "id_to_word = {i: word for word, i in word_to_id.items()}\n",
    "print(id_to_word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Will implment a complete tokenizer class that will handle the tokenization and encoding of the text\n",
    "\"\"\"\n",
    "This calss will have an encode method that splits text into tokens and carries out strings-to-integer mapping to produce token ids \n",
    "\n",
    "\n",
    "In additiona, we implement decode method that carries out the reverse mapping of the token ids back to the text tokens\n",
    "\n",
    "Step 1: Store the vocabulary as a class attribite for access in the encode and decode methods\n",
    "Step 2: Create an inverse vocabulary for the reverse mapping to the original text tokens\n",
    "Step 3: Process input text into token IDs\n",
    "Step 4: Convert token IDs back to text\n",
    "Step 5: Replace space before the specified punctuation marks\n",
    "\"\"\"\n",
    "\n",
    "class SimpleTokenizerV1:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_id = vocab\n",
    "        self.id_to_str = {i: word for word, i in vocab.items()}\n",
    "        \n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([.,!?:;\"()\\']|--|\\s)', text)\n",
    "        \n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "        ids = [self.str_to_id[token] for token in preprocessed]\n",
    "        return ids\n",
    "    \n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.id_to_str[i] for i in ids])\n",
    "        #Replacing spaces before the specified punctuation marks\n",
    "        text = re.sub(r'\\s([.,!?:;\"()\\'])', r'\\1', text)\n",
    "        return text\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 56, 2, 850, 988, 602, 533, 746, 5, 1126, 596, 5, 1, 67, 7, 38, 851, 1108, 754, 793, 7]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"For testing purpose, instantiate a new tokenizer object from SimpleTokenizerV1 class and encode and decode a sample text\"\"\"\n",
    "tokenizer = SimpleTokenizerV1(word_to_id)\n",
    "sample_text = \"\"\"\"It's the last he painted, you know,\" \n",
    "                    Mrs. Gisburn said with pardonable pride.\"\"\"\n",
    "encoded_ids = tokenizer.encode(sample_text)\n",
    "print(encoded_ids)\n",
    "#This block of code will prin the following token IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\" It' s the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.\n"
     ]
    }
   ],
   "source": [
    "#Converting the token IDs back to the original text\n",
    "decoded_text = tokenizer.decode(encoded_ids)\n",
    "print(decoded_text)\n",
    "\n",
    "#Test passed: Designed a toenizer capable of tokenizing and decoding text using the token IDs based on the snippet from the training dataset\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Hello'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[51], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#What if sentence present in the text is not present in the vocabulary?\u001b[39;00m\n\u001b[1;32m      3\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHello, do you like tea?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m#The problem here is that the word \"Hello\" is not present in the vocabulary.\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m#This highlights the need to consider large and diverse trainining sets to extend the vocabulary to cover all possible words in the text when working in LLMs\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m#This can be dealt by adding the special context token\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[48], line 24\u001b[0m, in \u001b[0;36mSimpleTokenizerV1.encode\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     21\u001b[0m preprocessed \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m([.,!?:;\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m()\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124m]|--|\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms)\u001b[39m\u001b[38;5;124m'\u001b[39m, text)\n\u001b[1;32m     23\u001b[0m preprocessed \u001b[38;5;241m=\u001b[39m [item\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m preprocessed \u001b[38;5;28;01mif\u001b[39;00m item\u001b[38;5;241m.\u001b[39mstrip()]\n\u001b[0;32m---> 24\u001b[0m ids \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstr_to_id\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m preprocessed]\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ids\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Hello'"
     ]
    }
   ],
   "source": [
    "#What if sentence present in the text is not present in the vocabulary?\n",
    "\n",
    "text = \"Hello, do you like tea?\"\n",
    "print(tokenizer.encode(text))\n",
    "\n",
    "#The problem here is that the word \"Hello\" is not present in the vocabulary.\n",
    "#This highlights the need to consider large and diverse trainining sets to extend the vocabulary to cover all possible words in the text when working in LLMs\n",
    "#This can be dealt by adding the special context token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Special Context Tokens\n",
    "\n",
    "In the previous code, we implemented a simple tokenizer and applied it to a passage from training set.\n",
    "\n",
    "Now we will modify the tokenizer to handle unknown words.\n",
    "\n",
    "In particular, we will modify the vocab and tokenizer we implemented in the previous section, SimpleTokenizerV2, to support two new tokens, <|unk|> and \n",
    "<|endoftext|>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1132\n"
     ]
    }
   ],
   "source": [
    "# We can modify the tokenizer to use an <|unk|> token if it encounters an unknown word or that is not present in the vocabulary\n",
    "#Furthermore, we also add the toke between the unrelated texts to separate them\n",
    "#For examle, when training GPT-like LLMs on multiple independent documents or books, it is common to insert a token before each document or book that follows a previous text source\n",
    "\n",
    "\n",
    "#Now we need to modify our vocab to include these two special tokens, and <|endoftext|>, by adding thse list of all uniqye words that is created in previous section\n",
    "\n",
    "all_tokens = sorted(set(preprocessed_text))\n",
    "all_tokens.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
    "\n",
    "vocab = {word: i for i, word in enumerate(all_tokens)}\n",
    "print(len(vocab.items()))\n",
    "\n",
    "#Without adding two tokens the length of voacabulary was lesser than 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('younger', 1127)\n",
      "('your', 1128)\n",
      "('yourself', 1129)\n",
      "('<|endoftext|>', 1130)\n",
      "('<|unk|>', 1131)\n"
     ]
    }
   ],
   "source": [
    "#As an additional quick check, printing the last 5 entries of the updated vocabulary\n",
    "for i, item in enumerate(vocab.items()):\n",
    "    if i >= len(vocab) - 5:\n",
    "        print(item)\n",
    "        \n",
    "#Thus we created a text tokenizer that handles the unknown words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Replace Unknow words by <|unk|> tokens\n",
    "# Step 2: Replace spaces before the specified punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV2:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_id = vocab\n",
    "        self.id_to_str = {i: word for word, i in vocab.items()}\n",
    "        \n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([.,!?:;\"()\\']|--|\\s)', text)\n",
    "        \n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "        preprocessed = [item if item in self.str_to_id \n",
    "               else \"<|unk|>\" for item in preprocessed]\n",
    "        ids = [self.str_to_id[token] for token in preprocessed]\n",
    "        \n",
    "        return ids\n",
    "    \n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.id_to_str[i] for i in ids])\n",
    "        #Replacing spaces before the specified punctuation marks\n",
    "        text = re.sub(r'\\s+([.,!?:;\"()\\'])', r'\\1', text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, do you like tea? <|endoftext|> In the sunlit terraces of the palace.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = SimpleTokenizerV2(vocab)\n",
    "text1 = \"Hello, do you like tea?\"\n",
    "text2 = \"In the sunlit terraces of the palace.\"\n",
    "\n",
    "#Joining two text sources together\n",
    "text = \" <|endoftext|> \".join((text1, text2))\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1131, 5, 355, 1126, 628, 975, 10, 1130, 55, 988, 956, 984, 722, 988, 1131, 7]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(text)\n",
    "\n",
    "#The output will be the token IDs for the text, with the <|unk|> token replacing the unknown word \"Hello\" in the first sentence\n",
    "\n",
    "#Now we don't have any error to worry about as we have added unknow token in vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|unk|>, do you like tea? <|endoftext|> In the sunlit terraces of the <|unk|>.'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Now we will be using the decode function now and pass the encoded text into the decoder \n",
    "tokenizer.decode(tokenizer.encode(text))\n",
    "\n",
    "#Here you will see the unknown text \"Hello\" is replaced by the <|unk|> token in the decoded text\n",
    "\n",
    "#We can confirm that the training datset didn't include the word hello and the palace.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes\n",
    "\n",
    "> Until now we have discussed tokenization as an essential step in processing text as input to LLMs. Depending on LLM, some researchers also considered adding the special tokens as following:\n",
    "\n",
    "[BOS] (begining of sequence): This tokens marks the start of the texts. It signifies to the LLM where a piece of content begins.\n",
    "\n",
    "[EOS] (end of sequence): This toeken is positioned at the end of the text, and is epsecially useful when concatenating multiple unrealted texts, similar to <|endoftext|>. For instance, when combining two different wikipedia articles or books, the [EOS] tokens indicates where one article ends and the other starts.\n",
    "\n",
    "[PAD] (padding): When training LLMs with batch sizes larger than one, teh batcj might contain texts of varying lengths. To ensure all texts have the same length, the shorter texts are extended or \"padded\" using [PAD] token, up to the length of the longest text in the batch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " > Note that the tokenizer used for GPT models do not need any of these tokens mentioned above but only uses  and <|endoftext|> token for simplicity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The tokenizer used for GPT models also doesn't use an |<unk>| token for outof-vocabulary words. Instead GPT model models uses a byte pair encoding tokenizer, which breaks down words to subwords units."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BYTE PAIR ENCODING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The previously implemented tokenization scheme in the previous section was for the illustration purpose only\n",
    "\n",
    "> Now we will go through more sophisticated tokenixaton scheme based on the concept called the byte pair encoding (BPE).\n",
    "`\n",
    "> The BPE tokenizer covered in this section was used to train LLMs such as GPT-2, GPT -3, and the original model used in chatGPT.\n",
    "\n",
    "> Since, implementing BPE can be relatively complicated, we will use ecisiting Python open-source library called tiktoken. This is a fast BPE tokeniser for use with OpenAI's models.\n",
    "\n",
    "> This library implements the BPE algorithm very efficiently based on the source code in RUST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15496, 11, 466, 345, 588, 8887, 30, 220, 50256, 554, 262, 4252, 18250, 8812, 2114, 286, 617, 34680, 27271, 13]\n"
     ]
    }
   ],
   "source": [
    "#Once installed tiktoken ibrary, we can use the WordPunctTokenizer class to tokenize the text\n",
    "import importlib\n",
    "import tiktoken\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "#The usage of this tokenizer is similar to the SimpleTokenizerV2 class we implemented previously via an encode method:\n",
    "\n",
    "text = (\"Hello, do you like tea? <|endoftext|> In the sunlit terraces of someunknownPlace.\" #Solves OOV problem\n",
    ")\n",
    "\n",
    "integers = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "#The code below will print the token IDs\n",
    "print(integers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, do you like tea? <|endoftext|> In the sunlit terraces of someunknownPlace.\n"
     ]
    }
   ],
   "source": [
    "#We can convert this token id back to the text using the decode method\n",
    "decoded_text = tokenizer.decode(integers)\n",
    "print(decoded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes\n",
    "\n",
    "We can make two noteworthy observation sbased on the token IDs and decode text above.\n",
    "\n",
    "> First, the <|endoftext|> token is assigned a relatively large token ID, namely 50256. In fact the BPE tokenizer, which was used to train models such as GPT-2, GPT-3, and the original model used in ChatGPT has, total vocab size of 50257, with <|endoftext|> being assigned the largest token ID.\n",
    "\n",
    "> Second, the BPE tokenizer above encodes and decodes unknown words, such as \"someunknownPlace\" correctly. The BPE tokenizer can handle any unknown words. How does it achieve this without using the <|unk|> tokens?\n",
    "\n",
    "> This is so because, the alogorithm underlying BPE breakd down the words that aren't in its predefined vocabulary into smaller subword units or even the individual characters. This enables it to handle the out of vocabulary words. So, thanks to the BPE algorithm, if the tokenizer encounters an unfamililar words during tokenization, it can represent it as a sequence of subword tokens or characters.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CREATING INPUT TARGET PAIRS\n",
    "\n",
    "> In this section, we will implement a data loader that fetches the input target pairs using a sliding window approach.\n",
    "\n",
    "> To get started, we will first tokenize the whole The Verdict short story we worked with earlier using the BPE tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5145\n"
     ]
    }
   ],
   "source": [
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "    \n",
    "enc_text = tokenizer.encode(raw_text)\n",
    "print(len(enc_text))\n",
    "\n",
    "#This is the total number of tokens in the training set, after applying BPE tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5095\n"
     ]
    }
   ],
   "source": [
    "#Now, removing the first 50 tokens from the dataset for demonstration purposes as it results in slightly more interesting tesxt passage in the next step\n",
    "\n",
    "enc_sample = enc_text[50:]\n",
    "print(len(enc_sample))\n",
    "\n",
    "#One of the easiest and most intutive ways to create the input-target pairs for the nextword prediction task is to create two variables, x and y, when x\n",
    "#contains the input tokens and y contains the target tokens, which are input shifted by 1. Basically, sliding window approach.\n",
    "\n",
    "#Why the input and output array size is the same? This is the context size. Context size is how many words do you wanna give as a output to the model\n",
    "#to make its prediction. The context size determines how many tokens are included in the input.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: [290, 4920, 2241, 287]\n",
      "y:      [4920, 2241, 287, 257]\n"
     ]
    }
   ],
   "source": [
    "context_size = 4 #length of the input\n",
    "\"\"\" \n",
    "The context size of 4 means that the model will be trained to look at a sequence of 4 tokens to predict the next token in sequence.\n",
    "The input x is the first 4 token [1,2,3,4] and the target y is the next 4 token [2,3,4,5].\n",
    "\"\"\"\n",
    "\n",
    "x = enc_sample[:context_size]\n",
    "y = enc_sample[1:context_size + 1]\n",
    "\n",
    "print(f\"x: {x}\")\n",
    "print(f\"y:      {y}\")\n",
    "#This is how input and output pairs are constructed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context: [290] -> desired: 4920\n",
      "context: [290, 4920] -> desired: 2241\n",
      "context: [290, 4920, 2241] -> desired: 287\n",
      "context: [290, 4920, 2241, 287] -> desired: 257\n"
     ]
    }
   ],
   "source": [
    "#Processing the inputs along with the targest, which are the inputs shifted by one position, we can then create the next-word prediction task as follows:\n",
    "\n",
    "for i in range(1, context_size+1):\n",
    "    context = enc_sample[:i]\n",
    "    desired = enc_sample[i]\n",
    "    print(f\"context: {context} -> desired: {desired}\")\n",
    " \n",
    " #Everything left of the arrow (-->) refers to the input an LLM would recieve, and the token ID on the right side of the arrow represents the target token ID\n",
    " #that LLM is supposed to predict.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context:  and -> desired:  established\n",
      "context:  and established -> desired:  himself\n",
      "context:  and established himself -> desired:  in\n",
      "context:  and established himself in -> desired:  a\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, context_size+1):\n",
    "    context = enc_sample[:i]\n",
    "    desired = enc_sample[i]\n",
    "    print(f\"context: {tokenizer.decode(context)} -> desired: {tokenizer.decode([desired])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes\n",
    "\n",
    "> Now we have created the input-target pairs that we can turn into use for the LLM training for next step.\n",
    "\n",
    "> There's only one more task before we can turn the tokens into embeddings, implementing an efficient data loader that iterates over the input dataset and returns the inputs and targets as PyTorch tensors, whoch can be thought of multidimensional array.\n",
    "\n",
    "> In particular, we will be returning two tensors: an input tensors containing the text that LLM sees and the target tensors that includes the targets for the LLM to predict."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPLEMENTING A DATA LOADER\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For more ef"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes\n",
    "\n",
    "For the efficient data loader implementation, we will use PyTorch's built in Dataset and DataLoader classes.\n",
    "\n",
    "> Step 1: Tokenize the entire text.\n",
    "\n",
    "> Step 2: Use a sliding window to chunk the book into overlapping sequences of max_length.\n",
    "\n",
    "> Step 3: Return the total number of rows in the dataset.\n",
    "\n",
    "> Step 4: Return a single row from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, text, tokenizer, max_length, stride): #Stride is how much we slide for next input output batch\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "        \n",
    "        #Tokenize the entire text\n",
    "        token_ids = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "        \n",
    "        #Using a sliding window to chunk the book into overlapping sequences of max_lengt\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i:i+max_length]\n",
    "            target_chunk = token_ids[i+1:i+max_length+1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "            \n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x = self.input_ids[idx]\n",
    "        y = self.target_ids[idx]\n",
    "        \n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes\n",
    "\n",
    "> Based on the Pytorch Dataset class.\n",
    "\n",
    "> Defines how individual rows are feteched from the dataset.\n",
    "\n",
    "> Each row consisits of a number of token IDs (based on max_length) assigned to the input_chunk tensor.\n",
    "\n",
    "> The target_chunk tensor contains the corresponding targets.\n",
    "\n",
    "> Recommendation: Look to see how data returned from the dataset looks like when we combined the dataset with Pytorch DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code will use the GPTDatasetV1 to load the inputs in batches via a PyTorch DataLoader:\n",
    "\n",
    "> Step 1: Initialize the tokenizer\n",
    "\n",
    "> Step 2: Create a Dataset\n",
    "\n",
    "> Step 3: drop_last = True drops the last batch if it is shorter than the specified batch_size to prevent loss spikes during training.\n",
    "\n",
    "> Step 4: The number of CPU processes to use for preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_loader_v1(txt, batch_size = 4, max_length = 256, stride= 128, shuffle=True, drop_last=True, num_workers=0):\n",
    "    #Initializing the tokenizer\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "    \n",
    "    #Creating the dataset\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
    "    \n",
    "    #Creating the DataLoader\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last, num_workers=num_workers)\n",
    "    \n",
    "    return dataloader\n",
    "\n",
    "#We will test the dataloadee with a batch size of 1 for an LLM with context size of 4.as_integer_ratio\n",
    "\n",
    "#This will develop and intuition fo how the GPTDataSetV1 class and create_data_loader_v1 function work togther"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "    \n",
    "#Convert dataloader into a Python iterator to fetch the next entry via Python's built-in next function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pytorch version:  2.6.0\n",
      "[tensor([[  40,  367, 2885, 1464]]), tensor([[ 367, 2885, 1464, 1807]])]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"Pytorch version: \", torch.__version__)\n",
    "dataloader = create_data_loader_v1(raw_text, batch_size=1, max_length=4, stride=1, shuffle=False)\n",
    "\n",
    "data_iter = iter(dataloader)\n",
    "first_batch = next(data_iter)\n",
    "print(first_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes\n",
    " \n",
    "> The first_batch variable contains two tensors: the first tensors store the input token IDs, and the second tensors store the target tokens IDs.\n",
    "\n",
    "> Since, the max_length is set to 4, each of the two tensors contains 4 token IDs.\n",
    "\n",
    "> Note: An input size of 4 is relativel small and only chosen for illustration purpose. It is common to train LLM with th input size of atlears 256.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[ 367, 2885, 1464, 1807]]), tensor([[2885, 1464, 1807, 3619]])]\n"
     ]
    }
   ],
   "source": [
    "second_batch = next(data_iter)\n",
    "print(second_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> If we compare the first with the second batch, we can see that the second batch's token ID is shifted by one position compared to the first batch.\n",
    "\n",
    "> The stride setting dictates the number of positions the input shifts across the bacteches, emulating a sliding window approach.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
